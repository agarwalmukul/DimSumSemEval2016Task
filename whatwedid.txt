In order to predict MWEs and supersenses better, we first added some basic features including word, pos and lemma sequences up to length 3 as well parent distance and pos.  This was used to include basic context information about the sentence and possible phrase and dependencies.  We also used wordnet. We peeked ahead the word to see if there was an exact match of any known MWE, as well as geting the current word's supersense (although not every word was covered by wordnet).  We checked for things like whether the word and words nearby it were capitalized since this might be indication of it being a name or entity or number. 

To update the loss function, we first removed the option of AUTO_HAMMING_LOSS and then kept a confusion matrix for each sentence.  The confusion matrix was used to tally the number of mwes that were falsely and accurately predicted in each sentence. We then calculated the f-score by calculating the prec and recall from the total number of fp, tp, and fn in the sentence (and then found the f-score from the typical equation using precision and recall).  We then used that score to update the loss with sch.loss(1-fscore) at the end of prediciton the entire sentence.

In addition to adding the features we talked about above, we also adjusted several methods to include supersense prediction. The `compute_reference` method was adjusted to return an oracle that included a supersense and the valid next method was adjusted so it would return valid joint predictions. We also had to create a number scheme to map the different ss and MWE pairs to a unique numerical value. In `compute_reference` and `valid_next` we assumed that words tagged `B` and `O` were the only words that could be tagged with a supersense.  This seemed to get the best results on the dev set. 

When we then tried to add the LDF features, we changed the `sch.set_options` call to include `is_ldf`.  We also changed the initialization of the vw instance so that it included `search=0` and `csooa_ldf=m`. Then for each prediction we created multiple examples instead of just one where each was labeled by the mwe and ss pair.  Then when we called predict we used the set of examples as examples.

For the full dimsum task, we expanced the bio class again. Compute_reference became much more complicated as the number of off track sequences increased alot. They were also a lot harder to resolve.  What we here was just a start and assume that all off track sequences should sync with the annotation. 
